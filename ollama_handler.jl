using JSON

FINAL_FILE = "prompts/final_instructions.json"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Load the final instruction JSON
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function load_final_instructions()
    if !isfile(FINAL_FILE)
        error("âŒ No final_instructions.json found. Please run the builder first.")
    end
    return JSON.parsefile(FINAL_FILE)
end

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. Combine system + user message
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function build_prompt(user_message::String)
    data = load_final_instructions()

    sys = """
    SYSTEM INSTRUCTION:
    $(data["base_prompt"])

    Tone: $(data["general_rules"]["tone"])
    Format: $(data["general_rules"]["format"])
    Never do: $(join(data["general_rules"]["never"], ", "))

    User Preferences:
    Style: $(get(data["user_customization"], "preferred_style", "neutral"))
    Focus: $(get(data["user_customization"], "focus", "general"))
    Difficulty: $(get(data["user_customization"], "difficulty", "normal"))

    $(data["final_instruction"])
    """

    prompt = """
    $sys

    USER INPUT:
    $user_message
    """

    return prompt
end

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. Send to Ollama (locally)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function query_ollama(user_message::String)
    data = load_final_instructions()
    model = data["model"]
    prompt = build_prompt(user_message)

    println("\nğŸ§  Generating response from Ollama ($model)...\n")

    try
        run(pipeline(`ollama run $model`, stdin=IOBuffer(prompt)))
    catch e
        println("âš ï¸ Error communicating with Ollama: ", e)
    end
end
