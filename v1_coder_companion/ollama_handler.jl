using JSON

FINAL_FILE = "./prompts/final_instructions.json"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Load the final instruction JSON
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function load_final_instructions()
    while !isfile(FINAL_FILE)
        error("âŒ No final_instructions.json found. Please run the builder first.")
        include("instruction_builder.jl")
    end
    return JSON.parsefile(FINAL_FILE)
end

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. Combine system + user message
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function build_prompt(user_message::String)
    data = load_final_instructions()

    sys = """
        $(data["base_prompt"])

        ğŸ’¬ Tone: $(data["general_rules"]["tone"])
        ğŸ§± Format: $(data["general_rules"]["format"])
        ğŸš« Never do: $(join(data["general_rules"]["never"], ",\n"))

        ğŸ¨ User Preferences:
        â€¢ Style â†’ $(get(data["user_customization"], "preferred_style", "neutral"))
        â€¢ Focus â†’ $(get(data["user_customization"], "focus", "general"))
        â€¢ Difficulty â†’ $(get(data["user_customization"], "difficulty", "normal"))

        ğŸ’» Language Context:
        $(data["language_prompt"])

        ğŸ“˜ Focus Area:
        $(data["genre_prompt"])

        âœ¨ Final Instruction:
        $(data["final_instruction"])

        ğŸ”¹ Remember:
        You are Bluna AI â€” a calm, intelligent, and adaptive programming companion. 
        Your responses must be clear, step-by-step, and aligned with the user's selected preferences.
"""


    prompt = """
    $sys

    USER INPUT:
    $user_message
    """

    return prompt
end

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. Send to Ollama (locally)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
function query_ollama(user_message::String)
    data = load_final_instructions()
    model = data["model"]
    prompt = build_prompt(user_message)

    println("\nğŸ§  Generating response from Ollama ($model)...\n")

    try
        run(pipeline(`ollama run $model`, stdin=IOBuffer(prompt)))
    catch e
        println("âš ï¸ Error communicating with Ollama: ", e)
    end
end
